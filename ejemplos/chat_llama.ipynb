{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ed0295-6f5e-4b2f-9601-c3239ca3652a",
   "metadata": {},
   "source": [
    "# Chat con Llama y Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fc84b-0815-4f40-99ab-d9a5da6bda91",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58394bf-1e45-46af-9bfd-01e24da6f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st #la librería para manejar la interfaz\n",
    "import ollama #la librería para trabajar con los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be12832-c01a-46f2-a39c-4cd22516e423",
   "metadata": {},
   "source": [
    "## Funciones para ayudar con el trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f08bd4-79d2-410e-9ea5-acf00f544dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inicializamos los parámetros de la aplicación\n",
    "def initialize():\n",
    "    st.set_page_config( #configuración básica\n",
    "        page_title=\"Chatbot con Ollama y Llama3.1\",\n",
    "        layout=\"wide\",\n",
    "    )\n",
    "\n",
    "#generamos la respuesta del modelo\n",
    "def generateResponse(chat_completion): #chat completion es el generador\n",
    "    for chunk in chat_completion: #cada chunk es un fragemento de la respuesta (no necesiriamente tokens)        \n",
    "        if chunk['message']['content']: #si el contenido ni es nulo o vacío...\n",
    "            yield chunk['message']['content'] #yield va generando \"bajo demanda\", si tener que esperar a toda la respuesta de golpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a478a0c-2c53-48ff-869c-4d08199931e1",
   "metadata": {},
   "source": [
    "## Inicializamos el sistemna y comenzamos a trabajar con él"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdb8417-c5dc-44bc-9bee-2e059d162699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 17:08:22.634 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-03 17:08:22.641 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-03 17:08:22.641 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-03 17:08:22.642 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-03 17:08:22.642 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-03 17:08:22.643 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-03 17:08:22.643 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-03 17:08:22.644 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-03 17:08:23.320 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run S:\\Dev\\Anaconda3\\envs\\llms\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-02-03 17:08:23.321 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "initialize()\n",
    "\n",
    "models = ollama.list()['models'] #cargamos los modelos de Ollama que tenemos en nuestro equipo\n",
    "\n",
    "if \"messages\" not in st.session_state: #si la sesión está todavía a vacío\n",
    "    st.session_state.messages = [] #incializamos los mensajes de la sesión \n",
    "    system_prompt = \"\"\"Eres un chatbot que se llama Ayudante. Vas a responder a todas las preguntas que sepas. \n",
    "                        Indicarás claramente cuando no sepas responder algo. \n",
    "                        También preguntarás lo que necesites para responder adecuadamente.\n",
    "                        \"\"\"\n",
    "    st.session_state.messages.append({\"role\": \"system\", \"content\": system_prompt}) #añadimos el prompt del sistema\n",
    "\n",
    "#muestra el diálogo desde el principio\n",
    "with st.container(): #crea un contenedor en la interfaz de Streamlit (cuadro principal)\n",
    "    #messages es una lista de diccionarios donde cada diccionario representa un mensaje en el chat\n",
    "    for message in st.session_state.messages: #st.session_state se puede utilizar para guardar datos de ejecución (en este caso los mensajes de la conversación)\n",
    "        if message[\"role\"] != \"system\": #ocultamos el prompt de sistema\n",
    "            with st.chat_message(message[\"role\"]): #st.chat_message crea un contenedor diseñado para mostrar mensajes de chat. message[\"role\"] determina como se visualiza el mensaje dependiendo del rol\n",
    "                st.markdown(message[\"content\"]) #renderiza el mensaje como markdowm\n",
    "\n",
    "prompt = st.chat_input(\"¿Con qué te puedo ayudar?\")\n",
    "if prompt: #cada vez que se pulse INTRO en el cuadro de conversación\n",
    "    st.chat_message(\"user\").markdown(prompt) #se muestra el contenido del ÚLTIMO mensaje en el cuadro de conversación\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt}) #agregamos el mensaje introducido en el historial de conversaciones\n",
    "    \n",
    "    chat_completion = ollama.chat(\n",
    "         model = \"llama3.1\", #el modelo seleccionado en el desplegable\n",
    "         messages = st.session_state.messages, #le pasa todo el historial para tener el contexto\n",
    "         stream = True,\n",
    "    )\n",
    "    \n",
    "    with st.chat_message(\"assistant\"):            \n",
    "        response = generateResponse(chat_completion)\n",
    "        full_response = st.write_stream(response) #st.write_stream simula la escritura                          \n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response}) #agregamos la respuesta al historial de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c80b87-f7f8-476e-81d9-fd2c215787a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
