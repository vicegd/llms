{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ed0295-6f5e-4b2f-9601-c3239ca3652a",
   "metadata": {},
   "source": [
    "# RAG con Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fc84b-0815-4f40-99ab-d9a5da6bda91",
   "metadata": {},
   "source": [
    "## Iniciación del modelo de Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58394bf-1e45-46af-9bfd-01e24da6f49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vicente García\\AppData\\Local\\Temp\\ipykernel_73184\\839886171.py:5: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model = MODEL_NAME) #inicializamos el modelo de Ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡hola! ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama #se utiliza para interactuar con modelos Ollama\n",
    "\n",
    "MODEL_NAME = \"llama3.2\"\n",
    "\n",
    "llm = Ollama(model = MODEL_NAME) #inicializamos el modelo de Ollama\n",
    "response = llm.invoke(\"¡hola!\") \n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be12832-c01a-46f2-a39c-4cd22516e423",
   "metadata": {},
   "source": [
    "## Carga de la información de diferentes fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f08bd4-79d2-410e-9ea5-acf00f544dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "page_content='17\n",
      "I.S.S.N. 0717 - 2079\n",
      "CIENCIA Y ENFERMERIA X (1): 17-21, 2004\n",
      "ELABORACIÓN DE UN ARTÍCULO CIENTÍFICO DE INVESTIGACIÓN\n",
      "RESEARCH SCIENTIFIC ARTICLE: KNOW HOW\n",
      "ELENA HENRÍQUEZ FIERRO* y MARIA INÉS ZEPEDA GONZALEZ**\n",
      "RESUMEN\n",
      "El artículo presenta la forma de redactar correctamente un artículo científico como reporte de una investigación,\n",
      "contempla cada una de las etapas que debe contener para su aprobación. Se sugiere la forma de redactar desde el título\n",
      "hasta la bibliografía, en un lenguaje comprensible y científico. Enfatiza en cómo deben ser presentados los resultados\n",
      "obtenidos para su mejor comprensión de la comunidad científica.\n",
      "Palabras claves: Artículo científico, producción, investigación.\n",
      "ABSTRACT\n",
      "This article introduces ways of writing correctly an article as a research report. It includes every step that has to be\n",
      "taken into account for its approval. The authors provide suggestions of the report writing going from the title to the\n",
      "references, both, in a comprehensive and scientific language. It emphasizes on how the results must be presented for a\n",
      "better understanding of the scientific community.\n",
      "Keywords: Scientific article, production, research\n",
      "Recepcionado: 10.05.2004.    Aceptado:11.06.2004\n",
      "INTRODUCCIÓN\n",
      "El objetivo del presente artículo es guiar a\n",
      "quienes se inician en la escritura científica,\n",
      "paso a paso en sus etapas, destacando los as-\n",
      "pectos más relevantes.\n",
      "La etapa final de una investigación es co-\n",
      "municar los resultados, de manera que éstos\n",
      "permitan integrar los conocimientos a la prác-\n",
      "tica profesional, es decir, se basa en los hallaz-\n",
      "gos de estudios científicos que deben tener va-\n",
      "lidez, importancia, novedad y utilidad para el\n",
      "quehacer profesional. En relación a las publi-\n",
      "caciones científicas, existe una serie de mitos:\n",
      "se cree, por ejemplo, que si una revista es de\n",
      "prestigio, publicar en ella es signo de validez\n",
      "o que si el autor es prestigioso la publicación\n",
      "será de gran valor; se piensa que si un autor\n",
      "ha publicado con anterioridad un muy buen\n",
      "trabajo, ello es garantía futura de validez cien-\n",
      "tífica o que está libre de prejuicios y de sesgos.\n",
      "Esto no es tan cierto, en el trabajo investigati-\n",
      "vo y sus resultados pesan muchos otros ele-\n",
      "mentos que se presentan en este artículo.\n",
      "Esta etapa final es similar a la etapa inicial\n",
      "en el grado de dificultad que involucra. Para\n",
      "interpretar y comunicar los resultados de un\n",
      "estudio se requiere experiencia, conocimiento\n",
      "de la estadística y capacidad de análisis para\n",
      "realizar los comentarios pertinentes relacio-\n",
      "nándolo con otros hallazgos de investigacio-\n",
      "nes similares, incluyendo creatividad del autor\n",
      "o autores, compenetración intelectual, razo-\n",
      "namiento lógico y sensibilidad frente a las in-\n",
      "terpretaciones que se pueden dar.\n",
      "*Enfermera, docente Departamento Enfermería Universidad de Concepción, Concepción, Chile. E-mail: ehenriqu@udec.cl\n",
      "**Enfermera, docente Departamento Enfermería Universidad de Concepción, Concepción, Chile. E-mail: marceped@udec.cl' metadata={'producer': 'Acrobat Distiller 5.0.5 for Macintosh', 'creator': 'PageMaker 6.5', 'creationdate': '2004-07-02T12:13:01-04:00', 'source': 'recursos/paper.pdf', 'file_path': 'recursos/paper.pdf', 'total_pages': 6, 'format': 'PDF 1.4', 'title': 'REVISTA', 'author': '2', 'subject': '', 'keywords': '', 'moddate': '2004-07-02T12:13:16-04:00', 'trapped': '', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader #para cargar documentos PDF\n",
    "\n",
    "loader = PyMuPDFLoader(\"recursos/paper.pdf\")\n",
    "data_pdf = loader.load() #lee el archivo PDF y lo convierte en una lista de objetos Document\n",
    "print(len(data_pdf)) #cada elemento de la lista data_pdf representa una página del PDF\n",
    "print(data_pdf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a478a0c-2c53-48ff-869c-4d08199931e1",
   "metadata": {},
   "source": [
    "## Creación de fragmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abdb8417-c5dc-44bc-9bee-2e059d162699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #se encarga de dividir elementos grandes y fragmentos más pequeños, teniendo en cuenta el texto para tratar de crear fragmentos coherentes\n",
    "\n",
    "#esto se hace porque los LLM tienen un límite de la cantidad de texto que pueden procesar y porque ayuda al procesamiento interno por parte de los modelos\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500) #el tamaño máximo de cada fragmento es 2000 caracteres en este caso. Se superpondrán 500 caracteres entre fragmentos consecutivos, para ayudar a mantener el contexto y la coherencia entre los fragmentos\n",
    "docs = text_splitter.split_documents(data_pdf) #se realiza la división de fragmentos\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e8d78-ce4c-4b05-aa8e-17050c82bb47",
   "metadata": {},
   "source": [
    "## Creación de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0275b1b-7cfe-4f9d-abfa-7650d378da0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name='sentence-transformers/all-MiniLM-L6-v2' max_length=512 cache_dir=None threads=None doc_embed_type='default' batch_size=256 parallel=None model=<fastembed.text.text_embedding.TextEmbedding object at 0x0000024E56AF65D0>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings #una clase para crear embeddings\n",
    "\n",
    "embed_model = FastEmbedEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") #es un modelo preentrenado para generar embeddings de texto\n",
    "\n",
    "print(embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845689d-c476-48ae-8b66-034937d03986",
   "metadata": {},
   "source": [
    "## Carga de los datos en Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49f1f1b-d9b8-45ac-9f3f-51118aee23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma #implementación de un almaceén de vectores que utiliza la base de datos ChromeDB internamente\n",
    "\n",
    "vs = Chroma.from_documents( #crea el almacén de vectores \n",
    "    documents = docs, #necesita la lista de documentos  de entrada\n",
    "    embedding = embed_model, #necesita el modelo de embeddings que se va a utilizar para convertir los documentos en vectores\n",
    "    persist_directory = \"chroma_db_dir\", #necesita una ruta para guardar los datos del almacén de vectores y así persistirlos\n",
    "    collection_name = \"my_test\" #necesita que se le proporcione un nombre a la colección para identificarla de otras\n",
    ")\n",
    "\n",
    "retriever = vs.as_retriever(search_kwargs={'k':3}) #retriever se utiliza para buscar documentos en el almacén de vectores. Con k : 3 le decimos que recupere los 3 elementos más similares (k vecinos) para cada consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28e98c1-f41e-46ea-bc1c-d2e5d7024f7d",
   "metadata": {},
   "source": [
    "## Plantilla de respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab231d9-d4f8-43d8-bc7b-395bb8c05028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate #clase para crear plantillas de prompts \n",
    "\n",
    "#utilizamos una variable para darle contexto a la pregunta \n",
    "#utilizamos otra variable para colocar la pregunta que ha hecho el usuario\n",
    "#con las plantillas podemos reutilizar código. Una vez que la plantilla está cubierta esto es lo que le llega al LLM\n",
    "custom_prompt_template = \"\"\"Usa la siguiente información para responder:\n",
    "\n",
    "Contexto: {context}\n",
    "Pregunta: {question}\n",
    "\n",
    "Solo devuelve la respuesta correcta y nada más.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=custom_prompt_template, input_variables=['context', 'question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eedc17-c0d1-4721-add4-08e5578c634e",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c9f0297-2151-4b25-9737-865050102c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I y IV.\n",
      "La sección de resultados es fundamental en un artículo científico, ya que es donde se presentan los hallazgos del estudio, las conclusiones y los datos obtenidos. Es importante que los resultados sean claros, concisos y precisos, y que estén presentados de manera ordenada y fácil de entender para la comunidad científica.\n",
      "No se proporciona ninguna información para responder a esta pregunta.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA #permite recuperar información de la base de datos\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = llm,\n",
    "                                chain_type = \"stuff\", #la información recuperada se meterá directamente en el prompt que se le envía al LLM (variable context). También se meterá la pregunta (query)\n",
    "                                retriever = retriever,\n",
    "                                return_source_documents = True, #para entender de dónde viene la información\n",
    "                                chain_type_kwargs = {\"prompt\":prompt}) #la plantilla que se le pasó al inicio\n",
    "\n",
    "questions = [{\"query\": \"¿qué secciones tiene un artículo científico?\"},\n",
    "             {\"query\": \"¿qué importancia tiene la sección de resultados en un artículo?\"},\n",
    "             {\"query\": \"¿Cuántos artículos científicos se publicaron al año en 2012?\"}]\n",
    "\n",
    "for question in questions:\n",
    "    response = qa.invoke(question) #query se traducirá internamente en la plantilla por question\n",
    "    print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c6ded-4884-4ab3-b562-625d63d34cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
